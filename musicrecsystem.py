# -*- coding: utf-8 -*-
"""MusicRecSystem.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ghXEDI2C8UvWLCYoqXpc9zt5a61p9VrV

#import libraries
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import seaborn as sns
import plotly.express as px
import matplotlib.pyplot as plt
import warnings
import os
import spotipy
import plotly.express as px
import plotly.graph_objects as go
import tensorflow as tf
import pickle

from tensorflow import keras
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.cluster import KMeans, DBSCAN
from sklearn.manifold import TSNE
from kneed import KneeLocator
from collections import defaultdict
from scipy.spatial.distance import cdist
from sklearn.pipeline import Pipeline
from sklearn.decomposition import PCA
from sklearn.metrics import euclidean_distances, silhouette_score
from scipy.spatial.distance import cdist
from yellowbrick.target import FeatureCorrelation
from spotipy.oauth2 import SpotifyClientCredentials
from collections import defaultdict


"""# import data"""
warnings.filterwarnings("ignore")

data = pd.read_csv('data/data.csv')
genre_data = pd.read_csv('data/data_by_genres.csv')
year_data = pd.read_csv('data/data_by_year.csv')

# print(data.info())

feature_names = ['acousticness', 'danceability', 'energy', 'instrumentalness',
       'liveness', 'loudness', 'speechiness', 'tempo', 'valence','duration_ms','explicit','key','mode','year']

X, y = data[feature_names], data['popularity']

# Create a list of the feature names
features = np.array(feature_names)

# Instantiate the visualizer
visualizer = FeatureCorrelation(labels=features)

plt.rcParams['figure.figsize']=(20,20)
visualizer.fit(X, y)     # Fit the data to the visualizer
visualizer.show()

def get_decade(year):
    period_start = int(year/10) * 10
    decade = '{}s'.format(period_start)
    return decade

data['decade'] = data['year'].apply(get_decade)

sns.set(rc={'figure.figsize':(11 ,6)})
sns.countplot(data['decade'])

sound_features = ['acousticness', 'danceability', 'energy', 'instrumentalness', 'liveness', 'valence']
fig = px.line(year_data, x='year', y=sound_features)
fig.show()

top10_genres = genre_data.nlargest(10, 'popularity')

fig = px.bar(top10_genres, x='genres', y=['valence', 'energy', 'danceability', 'acousticness'], barmode='group')
fig.show()

"""clustering Genres with K-Means"""

# Create the pipeline using make_pipeline
cluster_pipeline = make_pipeline(StandardScaler(), KMeans(n_clusters=10, random_state=42))

# Assuming genre_data is a pandas DataFrame
X = genre_data.select_dtypes(include=[np.number])

# Fit the pipeline and predict in one step
genre_data['cluster'] = cluster_pipeline.fit_predict(X)

# If you need the fitted pipeline for later use
fitted_pipeline = cluster_pipeline

tsne_pipeline = Pipeline([('scaler', StandardScaler()), ('tsne', TSNE(n_components=2, verbose=1))])
genre_embedding = tsne_pipeline.fit_transform(X)
projection = pd.DataFrame(columns=['x', 'y'], data=genre_embedding)
projection['genres'] = genre_data['genres']
projection['cluster'] = genre_data['cluster']

fig = px.scatter(
    projection, x='x', y='y', color='cluster', hover_data=['x', 'y', 'genres'])
fig.show()

"""cluster songs with K-Means"""

song_cluster_pipeline = make_pipeline(
    StandardScaler(),
    KMeans(
          n_clusters=20,
        random_state=42,
        n_init='auto',
        init='k-means++',
        algorithm='elkan',
    )
)

# Assuming 'data' is a pandas DataFrame
X = data.select_dtypes(include=[np.number])

# Store column names if needed
number_cols = X.columns.tolist()

# Fit the pipeline and predict in one step
data['cluster_label'] = song_cluster_pipeline.fit_predict(X)

# If you need the fitted pipeline for later use
fitted_pipeline = song_cluster_pipeline

pca_pipeline = Pipeline([('scaler', StandardScaler()), ('PCA', PCA(n_components=2))])
song_embedding = pca_pipeline.fit_transform(X)
projection = pd.DataFrame(columns=['x', 'y'], data=song_embedding)
projection['title'] = data['name']
projection['cluster'] = data['cluster_label']

fig = px.scatter(
    projection, x='x', y='y', color='cluster', hover_data=['x', 'y', 'title'])
fig.show()

sp = spotipy.Spotify(auth_manager=SpotifyClientCredentials(client_id="f703fdebb75d41eebeafc7ba5f4be5dd", client_secret="f6b1a933666c4c9c834a1e09ebeff10f"))

def find_song(name, year):
    song_data = defaultdict()
    results = sp.search(q= 'track: {} year: {}'.format(name,year), limit=1)
    if results['tracks']['items'] == []:
        return None

    results = results['tracks']['items'][0]
    track_id = results['id']
    audio_features = sp.audio_features(track_id)[0]

    song_data['name'] = [name]
    song_data['year'] = [year]
    song_data['explicit'] = [int(results['explicit'])]
    song_data['duration_ms'] = [results['duration_ms']]
    song_data['popularity'] = [results['popularity']]

    for key, value in audio_features.items():
        song_data[key] = value

    return pd.DataFrame(song_data)

number_cols = ['valence', 'year', 'acousticness', 'danceability', 'duration_ms', 'energy', 'explicit',
               'instrumentalness', 'key', 'liveness', 'loudness', 'mode', 'popularity', 'speechiness', 'tempo']

def get_song_data(song, spotify_data):
    return spotify_data[(spotify_data['name'] == song['name']) &
                        (spotify_data['year'] == song['year'])].iloc[0]

def get_mean_vector(song_list, spotify_data):
    song_vectors = []
    for song in song_list:
        try:
            song_data = get_song_data(song, spotify_data)
            song_vector = song_data[number_cols].values
            song_vectors.append(song_vector)
        except IndexError:
            print(f"Warning: {song['name']} does not exist in database")
    return np.mean(song_vectors, axis=0) if song_vectors else None

def flatten_dict_list(dict_list):
    flattened_dict = defaultdict(list)
    for dictionary in dict_list:
        for key, value in dictionary.items():
            flattened_dict[key].append(value)
    return flattened_dict

def recommend_songs(song_list, spotify_data, n_songs=10):
    metadata_cols = ['name', 'year', 'artists']
    song_dict = flatten_dict_list(song_list)

    song_center = get_mean_vector(song_list, spotify_data)
    if song_center is None:
        return []

    song_cluster_pipeline.fit(spotify_data[number_cols])

    scaler = song_cluster_pipeline.named_steps['standardscaler']
    scaled_data = scaler.transform(spotify_data[number_cols])
    scaled_song_center = scaler.transform(song_center.reshape(1, -1))
    distances = cdist(scaled_song_center, scaled_data, 'cosine')
    index = np.argsort(distances)[0][:n_songs]

    rec_songs = spotify_data.iloc[index]
    rec_songs = rec_songs[~rec_songs['name'].isin(song_dict['name'])]
    return rec_songs[metadata_cols].to_dict(orient='records')

# prompt: call the recommend_songs function

song_list = [{'name': 'Location', 'year':2017}]

recommended_songs = recommend_songs(song_list, data)

artist_data = pd.read_csv('data/data_by_artist.csv')

#cluster the data
cluster_pipeline = make_pipeline(StandardScaler(), KMeans(n_clusters=20))
X = artist_data.select_dtypes(np.number)
cluster_pipeline.fit(X)
artist_data['cluster'] = cluster_pipeline.predict(X)

#plot the clusters
tsne_pipeline = make_pipeline(
    StandardScaler(),
    TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)
)
genre_embedding = tsne_pipeline.fit_transform(X)
projection = pd.DataFrame(columns=['x', 'y'], data=genre_embedding)
projection['name'] = artist_data['artists']
projection['cluster'] = artist_data['cluster']

fig = px.scatter(
    projection, x='x', y='y', color='cluster', hover_data=['x', 'y', 'name'])
fig.show()

def recommend_artists(artist_name, artist_data, data, n_artists=5):
    try:
        # Get cluster label for the input artist
        artist_cluster = artist_data[artist_data['artists'] == artist_name]['cluster'].iloc[0]

        # Find other artists in the same cluster
        recommended_artists = artist_data[artist_data['cluster'] == artist_cluster]['artists'].tolist()

        # Exclude the input artist
        recommended_artists.remove(artist_name)

        # Limit the number of recommendations
        return recommended_artists[:n_artists]

    except IndexError:
        print(f"Artist '{artist_name}' not found in the dataset.")
        return []

input_artist = 'Playboi Carti'  # Replace with desired artist
recommendations = recommend_artists(input_artist, artist_data, data)
print(f"Recommended artists for '{input_artist}': {recommendations}")

# Commented out IPython magic to ensure Python compatibility.

class ArtistClusterAnalyzer:
    def __init__(self, data):
        """
        Initialize the cluster analyzer with artist data

        Parameters:
        data (pd.DataFrame): DataFrame containing artist features
        """
        self.data = data
        self.X = data.select_dtypes(np.number)
        self.scaler = RobustScaler()  # More robust to outliers than StandardScaler
        self.X_scaled = self.scaler.fit_transform(self.X)

    def find_optimal_clusters(self, max_clusters=30):
        """
        Use the elbow method to find optimal number of clusters
        """
        inertias = []
        silhouette_scores = []

        for k in range(2, max_clusters + 1):
            kmeans = KMeans(n_clusters=k, random_state=42)
            kmeans.fit(self.X_scaled)
            inertias.append(kmeans.inertia_)
            silhouette_scores.append(silhouette_score(self.X_scaled, kmeans.labels_))

        # Find the elbow point
        kn = KneeLocator(
            range(2, max_clusters + 1), inertias,
            curve='convex', direction='decreasing'
        )

        # Plot elbow curve
        fig = go.Figure()
        fig.add_trace(go.Scatter(
            x=list(range(2, max_clusters + 1)),
            y=inertias,
            name='Inertia'
        ))
        fig.add_trace(go.Scatter(
            x=list(range(2, max_clusters + 1)),
            y=silhouette_scores,
            name='Silhouette Score',
            yaxis='y2'
        ))

        fig.update_layout(
            title='Elbow Method & Silhouette Analysis',
            xaxis_title='Number of Clusters',
            yaxis_title='Inertia',
            yaxis2=dict(
                title='Silhouette Score',
                overlaying='y',
                side='right'
            )
        )

        return kn.elbow, fig

    def perform_kmeans_clustering(self, n_clusters):
        """
        Perform KMeans clustering with the optimal number of clusters
        """
        self.kmeans = KMeans(n_clusters=n_clusters, random_state=42)
        self.cluster_labels = self.kmeans.fit_predict(self.X_scaled)
        return self.cluster_labels

    def perform_dbscan_clustering(self, eps=0.5, min_samples=5):
        """
        Perform DBSCAN clustering as an alternative
        """
        self.dbscan = DBSCAN(eps=eps, min_samples=min_samples)
        self.dbscan_labels = self.dbscan.fit_predict(self.X_scaled)
        return self.dbscan_labels

    def create_tsne_projection(self, perplexity=40, n_iter=1000):
        """
        Create t-SNE projection with improved parameters
        """
        tsne = TSNE(
            n_components=2,
            perplexity=perplexity,
            n_iter=n_iter,
            random_state=42,
            learning_rate='auto',
            init='pca'  # Use PCA initialization for better global structure preservation
        )

        self.embedding = tsne.fit_transform(self.X_scaled)
        return self.embedding

    def visualize_clusters(self, labels, method='kmeans'):
        """
        Create an interactive visualization of the clusters
        """
        projection = pd.DataFrame({
            'x': self.embedding[:, 0],
            'y': self.embedding[:, 1],
            'name': self.data['artists'],
            'cluster': labels
        })

        # Calculate cluster centers
        cluster_centers = pd.DataFrame({
            'x': [self.embedding[labels == i, 0].mean() for i in range(labels.max() + 1)],
            'y': [self.embedding[labels == i, 1].mean() for i in range(labels.max() + 1)]
        })

        # Create main scatter plot
        fig = px.scatter(
            projection,
            x='x',
            y='y',
            color='cluster',
            hover_data=['name'],
            title=f'Artist Clusters ({method.upper()})'
        )

        # Add cluster centers and labels
        for i in range(len(cluster_centers)):
            fig.add_trace(
                go.Scatter(
                    x=[cluster_centers.loc[i, 'x']],
                    y=[cluster_centers.loc[i, 'y']],
                    mode='markers+text',
                    marker=dict(symbol='x', size=15, color='black'),
                    text=[f'Cluster {i}'],
                    name=f'Center {i}'
                )
            )

        fig.update_layout(
            template='plotly_white',
            legend_title_text='Cluster',
            hovermode='closest'
        )

        return fig

# Usage example:
def analyze_artist_clusters(artist_data, max_clusters=30):
    """
    Perform complete clustering analysis on artist data
    """
    # Initialize analyzer
    analyzer = ArtistClusterAnalyzer(artist_data)

    # Find optimal number of clusters
    optimal_clusters, elbow_fig = analyzer.find_optimal_clusters(max_clusters)
    print(f"Optimal number of clusters: {optimal_clusters}")

    # Perform both clustering methods
    kmeans_labels = analyzer.perform_kmeans_clustering(optimal_clusters)
    dbscan_labels = analyzer.perform_dbscan_clustering()

    # Create t-SNE projection
    embedding = analyzer.create_tsne_projection()

    # Visualize both clustering results
    kmeans_fig = analyzer.visualize_clusters(kmeans_labels, 'kmeans')
    dbscan_fig = analyzer.visualize_clusters(dbscan_labels, 'dbscan')

    return {
        'analyzer': analyzer,
        'optimal_clusters': optimal_clusters,
        'elbow_plot': elbow_fig,
        'kmeans_plot': kmeans_fig,
        'dbscan_plot': dbscan_fig
    }

# artist_data = pd.read_csv('data_by_artist.csv')
# Load your data
artist_data = pd.DataFrame(artist_data)
# Perform the complete analysis
results = analyze_artist_clusters(artist_data)

# Access the results
results['optimal_clusters']  # Get the optimal number of clusters
results['elbow_plot'].show()  # View the elbow plot
results['kmeans_plot'].show()  # View the KMeans clustering results
results['dbscan_plot'].show()  # View the DBSCAN clustering results

# Access the analyzer for custom analysis
analyzer = results['analyzer']

# prep data
analyzer = results['analyzer']
# Prepare the data for the neural network
X = analyzer.X_scaled  # Scaled features
y = artist_data['artists'] # Artist names as labels

# Encode artist names to numerical labels
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.5, random_state=42)

# Define the neural network model
model = keras.Sequential([
    keras.layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),
    keras.layers.Dense(64, activation='relu'),
    keras.layers.Dense(len(label_encoder.classes_), activation='softmax')  # Output layer with softmax for classification
])

# Compile the model
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# Train the model
model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test)) # Adjust epochs and batch size as needed


def recommend_artists_nn(artist_name, artist_data, analyzer, model, label_encoder, n_artists=5):
    try:
      # Find the artist's features
      artist_features = artist_data[artist_data['artists'] == artist_name].select_dtypes(np.number)
      if artist_features.empty:
          print(f"Artist '{artist_name}' not found in the dataset.")
          return []

      # Scale the features
      scaled_features = analyzer.scaler.transform(artist_features)

      # Predict probabilities for all artists
      predictions = model.predict(scaled_features)

      # Get indices of the top N recommended artists (excluding the input artist)
      top_indices = np.argsort(predictions[0])[::-1][1:n_artists+1]  # Exclude input artist

      # Decode the indices back to artist names
      recommendations = [label_encoder.inverse_transform([i])[0] for i in top_indices]
      return recommendations

    except Exception as e:
      print(f"An error occurred: {e}")
      return []

def userinput():
    artist_name = input('Enter the artist name: ')
    recommendations = recommend_artists_nn(artist_name, artist_data, analyzer, model, label_encoder)
    return recommendations

input_artist = 'Playboi Carti'  # Replace with desired artist
recommendations = recommend_artists_nn(input_artist, artist_data, analyzer, model, label_encoder)
print(f"Recommended artists for '{input_artist}': {recommendations}")

input_artist = 'Junior H'  # Replace with desired artist
recommendations = recommend_artists_nn(input_artist, artist_data, analyzer, model, label_encoder)
print(f"Recommended artists for '{input_artist}': {recommendations}")

# save the model for faster use
model.save('artist_recommender_model.h5')

model = keras.models.load_model('artist_recommender_model.h5')
